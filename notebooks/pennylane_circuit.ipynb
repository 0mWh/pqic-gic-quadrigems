{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a919082-f21a-473c-b6f2-9d92f1a78b90",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cuda(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import environ as ENV\n",
    "ENV['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "ROOT = ENV['PWD']\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "Float = np.float64\n",
    "\n",
    "from itertools import combinations\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(jax.devices())\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import pennylane as qml\n",
    "dev = qml.device(\"lightning.gpu\", wires=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f1a244-0066-4b50-bb4b-1047613d0e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cb270a4-3829-403a-b681-677e95327038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user/work/quadrigems/.venv/bin/catalyst'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pennylane-catalyst uv/venv metagarbage\n",
    "import catalyst\n",
    "# https://github.com/PennyLaneAI/catalyst/pull/1839\n",
    "# catalyst.utils.runtime_environment.get_cli_path = lambda: ENV['PWD'] + '/.venv/bin/catalyst'\n",
    "catalyst.utils.runtime_environment.get_cli_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a981b7cd-63e1-405b-b0bd-06c2c6a446a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to delete cache when rewriting function\n",
    "import diskcache as dc\n",
    "cache = dc.Cache(\n",
    "    ROOT + '/.cache/pennylane_sim/',\n",
    "    size_limit = 2 ** 32, # 4GB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1dde430-9cce-4e8d-b4b2-eb6a6937fc7f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from circuit_postprocess import *\n",
    "from should_be_stdlib import *\n",
    "from neurodata import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca6e19b8-fc5d-4f2f-b34e-28800567893a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if either is the 0 vector, then norm is undefined. unlikely to occur\n",
    "@qml.qjit(seed=0)\n",
    "@qml.qnode(dev)\n",
    "def swap_circuit(data1:npt.NDArray[Float], data2:npt.NDArray[Float], norm:bool=True) -> npt.NDArray[Float]:\n",
    "    l1, l2 = len(data1), len(data2)\n",
    "    assert l1 == l2\n",
    "\n",
    "    # normalize input data\n",
    "    data1 = data1 / jnp.linalg.norm(data1, ord=1) * jnp.pi/2\n",
    "    data2 = data2 / jnp.linalg.norm(data2, ord=1) * jnp.pi/2\n",
    "    # print(data1)\n",
    "    # print(data2)\n",
    "\n",
    "    # data embedding\n",
    "    for i in range(l1 + l2):\n",
    "        qml.H(i)\n",
    "    for i in range(l1):\n",
    "        qml.RY(data1[i], i)\n",
    "    for i in range(l2):\n",
    "        qml.RY(data2[i], l1 + i)\n",
    "\n",
    "    # QFT is inv DFT; inv QFT is DFT\n",
    "    qml.QFT(wires = range(l1))\n",
    "    qml.QFT(wires = range(l1, l1 + l2))\n",
    "\n",
    "    # Swap test\n",
    "    for i in range(l1):\n",
    "        qml.CNOT(wires = [i, l1 + i])\n",
    "    qml.Barrier()\n",
    "    for i in range(l1):\n",
    "        qml.H(i)\n",
    "\n",
    "    return qml.probs(wires = range(l1 + l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa8e8000-75c1-4e32-b9cd-cdeec36644bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: memoize over the neuron idx\n",
    "\n",
    "from time import time as now\n",
    "\n",
    "# don't cache while debugging\n",
    "# @cache.memoize()\n",
    "def get_fidelity_memo(data1:list[int], data2:list[int]) -> tuple[Float,Float]:\n",
    "    probs = swap_circuit(data1, data2)\n",
    "    out = swap_expectation(probs, len(data1))\n",
    "    return out\n",
    "\n",
    "def get_fidelity(data1:list[int], data2:list[int]) -> tuple[Float,Float]:\n",
    "    # data1 should be less than data2\n",
    "    if is_array_lesser(data2, data1):\n",
    "        return get_fidelity(data2, data1)\n",
    "    # normalize data\n",
    "    data1 = data1 / np.linalg.norm(data1, ord=1) * np.pi/2\n",
    "    data2 = data2 / np.linalg.norm(data2, ord=1) * np.pi/2\n",
    "    return get_fidelity_memo(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca1b7bcd-0208-40d5-888e-f793fa540d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should get 1\n",
    "a = np.array([2,2,2, 2,2,2, 2,2,2,], dtype=Float)\n",
    "b = np.array([3,3,3, 3,3,3, 3,3,3,], dtype=Float)\n",
    "#normalization will make these equal to eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331e7d73-f05c-4bef-b444-5cbdf4533038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1744017f-d871-4573-ba61-d7ad48cd671f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drawing\n",
      " 0: ──H──RY(0.17)─╭QFT─╭●──────────────────────────||──H─┤ ╭Probs\n",
      " 1: ──H──RY(0.17)─├QFT─│──╭●───────────────────────||──H─┤ ├Probs\n",
      " 2: ──H──RY(0.17)─├QFT─│──│──╭●────────────────────||──H─┤ ├Probs\n",
      " 3: ──H──RY(0.17)─├QFT─│──│──│──╭●─────────────────||──H─┤ ├Probs\n",
      " 4: ──H──RY(0.17)─├QFT─│──│──│──│──╭●──────────────||──H─┤ ├Probs\n",
      " 5: ──H──RY(0.17)─├QFT─│──│──│──│──│──╭●───────────||──H─┤ ├Probs\n",
      " 6: ──H──RY(0.17)─├QFT─│──│──│──│──│──│──╭●────────||──H─┤ ├Probs\n",
      " 7: ──H──RY(0.17)─├QFT─│──│──│──│──│──│──│──╭●─────||──H─┤ ├Probs\n",
      " 8: ──H──RY(0.17)─╰QFT─│──│──│──│──│──│──│──│──╭●──||──H─┤ ├Probs\n",
      " 9: ──H──RY(0.17)─╭QFT─╰X─│──│──│──│──│──│──│──│───||────┤ ├Probs\n",
      "10: ──H──RY(0.17)─├QFT────╰X─│──│──│──│──│──│──│───||────┤ ├Probs\n",
      "11: ──H──RY(0.17)─├QFT───────╰X─│──│──│──│──│──│───||────┤ ├Probs\n",
      "12: ──H──RY(0.17)─├QFT──────────╰X─│──│──│──│──│───||────┤ ├Probs\n",
      "13: ──H──RY(0.17)─├QFT─────────────╰X─│──│──│──│───||────┤ ├Probs\n",
      "14: ──H──RY(0.17)─├QFT────────────────╰X─│──│──│───||────┤ ├Probs\n",
      "15: ──H──RY(0.17)─├QFT───────────────────╰X─│──│───||────┤ ├Probs\n",
      "16: ──H──RY(0.17)─├QFT──────────────────────╰X─│───||────┤ ├Probs\n",
      "17: ──H──RY(0.17)─╰QFT─────────────────────────╰X──||────┤ ╰Probs\n"
     ]
    }
   ],
   "source": [
    "print('Drawing')\n",
    "print(qml.draw(swap_circuit)(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c9c3d8-cf98-49f8-916f-546a3320c05f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(0.88606496, dtype=float64), Array(0.94303248, dtype=float64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_swap, fidelity = get_fidelity(a, b)\n",
    "exp_swap, fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e64af1-1efa-4eb6-a424-be4cddfafa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTING:\n",
    "    0 / 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb40fc1-a7ab-48d7-8d5e-371c0d647853",
   "metadata": {},
   "source": [
    "# run simulated results on significant neurons of smallest recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c3e2211-eca8-47aa-ae57-0a6df7deacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = load_record(DEFAULT_RECORD)\n",
    "sig_neurons = get_sig_neurons(record)\n",
    "tuning_curves = get_tuning_curves(record).loc[sig_neurons]\n",
    "coords = get_coords(record).loc[sig_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21254157-ca39-4654-a9e3-ac2baa06b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    quantum_fidelity = pd.read_csv(ROOT + '/data/quantum_fidelity.csv')\n",
    "\n",
    "except:\n",
    "    quantum_fidelity = pd.DataFrame(columns = ['A', 'B', 'f', 'fidelity'])\n",
    "\n",
    "    # A x B\n",
    "    pairs, pairs_len = combinations(sig_neurons, 2), (len(sig_neurons) * (len(sig_neurons) - 1) // 2)\n",
    "    for a, b in tqdm(pairs, total=pairs_len):\n",
    "        a_i, b_i = tuning_curves.loc[a], tuning_curves.loc[b]\n",
    "        f, fidelity = get_fidelity(a_i, b_i)\n",
    "        quantum_fidelity.loc[len(quantum_fidelity)] = [a, b, f, fidelity]\n",
    "        quantum_fidelity.loc[len(quantum_fidelity)] = [b, a, f, fidelity]\n",
    "\n",
    "    # A x A = 1 ??? not working\n",
    "    for a in tqdm(sig_neurons):\n",
    "        a_i = tuning_curves.loc[a]\n",
    "        f, fidelity = get_fidelity(a_i, a_i)\n",
    "        quantum_fidelity.loc[len(quantum_fidelity)] = [a, a, f, fidelity]\n",
    "\n",
    "    quantum_fidelity.to_csv(ROOT + '/data/quantum_fidelity.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51643e19-fc4e-408a-a1c5-50f7cd4ef2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantum_fidelity_matrix = quantum_fidelity.pivot_table(index = 'A', columns = 'B')['fidelity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5fafc5d-2f9a-46d0-9d28-ea2babd7381f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>B</th>\n",
       "      <th>23</th>\n",
       "      <th>32</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>50</th>\n",
       "      <th>53</th>\n",
       "      <th>55</th>\n",
       "      <th>57</th>\n",
       "      <th>59</th>\n",
       "      <th>...</th>\n",
       "      <th>411</th>\n",
       "      <th>416</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>428</th>\n",
       "      <th>435</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.940140</td>\n",
       "      <td>0.934226</td>\n",
       "      <td>0.940871</td>\n",
       "      <td>0.946093</td>\n",
       "      <td>0.940307</td>\n",
       "      <td>0.940192</td>\n",
       "      <td>0.941881</td>\n",
       "      <td>0.940328</td>\n",
       "      <td>0.938246</td>\n",
       "      <td>0.940784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941057</td>\n",
       "      <td>0.940519</td>\n",
       "      <td>0.940024</td>\n",
       "      <td>0.939312</td>\n",
       "      <td>0.926623</td>\n",
       "      <td>0.937239</td>\n",
       "      <td>0.940583</td>\n",
       "      <td>0.940746</td>\n",
       "      <td>0.939575</td>\n",
       "      <td>0.941619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.934226</td>\n",
       "      <td>0.928966</td>\n",
       "      <td>0.934924</td>\n",
       "      <td>0.940397</td>\n",
       "      <td>0.934481</td>\n",
       "      <td>0.934343</td>\n",
       "      <td>0.936011</td>\n",
       "      <td>0.934490</td>\n",
       "      <td>0.932356</td>\n",
       "      <td>0.934927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935213</td>\n",
       "      <td>0.934719</td>\n",
       "      <td>0.934431</td>\n",
       "      <td>0.933643</td>\n",
       "      <td>0.920515</td>\n",
       "      <td>0.931490</td>\n",
       "      <td>0.934711</td>\n",
       "      <td>0.934883</td>\n",
       "      <td>0.933594</td>\n",
       "      <td>0.935780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.940871</td>\n",
       "      <td>0.934924</td>\n",
       "      <td>0.941590</td>\n",
       "      <td>0.946788</td>\n",
       "      <td>0.941022</td>\n",
       "      <td>0.940913</td>\n",
       "      <td>0.942596</td>\n",
       "      <td>0.941047</td>\n",
       "      <td>0.938968</td>\n",
       "      <td>0.941507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941788</td>\n",
       "      <td>0.941233</td>\n",
       "      <td>0.940735</td>\n",
       "      <td>0.940030</td>\n",
       "      <td>0.927362</td>\n",
       "      <td>0.937931</td>\n",
       "      <td>0.941308</td>\n",
       "      <td>0.941468</td>\n",
       "      <td>0.940312</td>\n",
       "      <td>0.942332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.946093</td>\n",
       "      <td>0.940397</td>\n",
       "      <td>0.946788</td>\n",
       "      <td>0.952274</td>\n",
       "      <td>0.946297</td>\n",
       "      <td>0.946138</td>\n",
       "      <td>0.947873</td>\n",
       "      <td>0.946284</td>\n",
       "      <td>0.944138</td>\n",
       "      <td>0.946763</td>\n",
       "      <td>...</td>\n",
       "      <td>0.947053</td>\n",
       "      <td>0.946523</td>\n",
       "      <td>0.946100</td>\n",
       "      <td>0.945386</td>\n",
       "      <td>0.932182</td>\n",
       "      <td>0.943127</td>\n",
       "      <td>0.946538</td>\n",
       "      <td>0.946726</td>\n",
       "      <td>0.945495</td>\n",
       "      <td>0.947633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.940307</td>\n",
       "      <td>0.934481</td>\n",
       "      <td>0.941022</td>\n",
       "      <td>0.946297</td>\n",
       "      <td>0.940486</td>\n",
       "      <td>0.940379</td>\n",
       "      <td>0.942053</td>\n",
       "      <td>0.940523</td>\n",
       "      <td>0.938426</td>\n",
       "      <td>0.940951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941227</td>\n",
       "      <td>0.940697</td>\n",
       "      <td>0.940256</td>\n",
       "      <td>0.939516</td>\n",
       "      <td>0.926735</td>\n",
       "      <td>0.937473</td>\n",
       "      <td>0.940759</td>\n",
       "      <td>0.940910</td>\n",
       "      <td>0.939703</td>\n",
       "      <td>0.941787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.937239</td>\n",
       "      <td>0.931490</td>\n",
       "      <td>0.937931</td>\n",
       "      <td>0.943127</td>\n",
       "      <td>0.937473</td>\n",
       "      <td>0.937313</td>\n",
       "      <td>0.938978</td>\n",
       "      <td>0.937447</td>\n",
       "      <td>0.935388</td>\n",
       "      <td>0.937896</td>\n",
       "      <td>...</td>\n",
       "      <td>0.938154</td>\n",
       "      <td>0.937660</td>\n",
       "      <td>0.937137</td>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.923919</td>\n",
       "      <td>0.934393</td>\n",
       "      <td>0.937663</td>\n",
       "      <td>0.937884</td>\n",
       "      <td>0.936693</td>\n",
       "      <td>0.938737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.940583</td>\n",
       "      <td>0.934711</td>\n",
       "      <td>0.941308</td>\n",
       "      <td>0.946538</td>\n",
       "      <td>0.940759</td>\n",
       "      <td>0.940645</td>\n",
       "      <td>0.942326</td>\n",
       "      <td>0.940779</td>\n",
       "      <td>0.938703</td>\n",
       "      <td>0.941237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941506</td>\n",
       "      <td>0.940979</td>\n",
       "      <td>0.940513</td>\n",
       "      <td>0.939784</td>\n",
       "      <td>0.927082</td>\n",
       "      <td>0.937663</td>\n",
       "      <td>0.941037</td>\n",
       "      <td>0.941178</td>\n",
       "      <td>0.940019</td>\n",
       "      <td>0.942065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>0.940746</td>\n",
       "      <td>0.934883</td>\n",
       "      <td>0.941468</td>\n",
       "      <td>0.946726</td>\n",
       "      <td>0.940910</td>\n",
       "      <td>0.940801</td>\n",
       "      <td>0.942487</td>\n",
       "      <td>0.940945</td>\n",
       "      <td>0.938826</td>\n",
       "      <td>0.941386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941676</td>\n",
       "      <td>0.941113</td>\n",
       "      <td>0.940623</td>\n",
       "      <td>0.939936</td>\n",
       "      <td>0.927098</td>\n",
       "      <td>0.937884</td>\n",
       "      <td>0.941178</td>\n",
       "      <td>0.941380</td>\n",
       "      <td>0.940159</td>\n",
       "      <td>0.942223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>0.939575</td>\n",
       "      <td>0.933594</td>\n",
       "      <td>0.940312</td>\n",
       "      <td>0.945495</td>\n",
       "      <td>0.939703</td>\n",
       "      <td>0.939618</td>\n",
       "      <td>0.941296</td>\n",
       "      <td>0.939755</td>\n",
       "      <td>0.937674</td>\n",
       "      <td>0.940198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940489</td>\n",
       "      <td>0.939917</td>\n",
       "      <td>0.939436</td>\n",
       "      <td>0.938680</td>\n",
       "      <td>0.926062</td>\n",
       "      <td>0.936693</td>\n",
       "      <td>0.940019</td>\n",
       "      <td>0.940159</td>\n",
       "      <td>0.939000</td>\n",
       "      <td>0.941022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.941619</td>\n",
       "      <td>0.935780</td>\n",
       "      <td>0.942332</td>\n",
       "      <td>0.947633</td>\n",
       "      <td>0.941787</td>\n",
       "      <td>0.941676</td>\n",
       "      <td>0.943365</td>\n",
       "      <td>0.941820</td>\n",
       "      <td>0.939712</td>\n",
       "      <td>0.942263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.942546</td>\n",
       "      <td>0.942002</td>\n",
       "      <td>0.941553</td>\n",
       "      <td>0.940827</td>\n",
       "      <td>0.927970</td>\n",
       "      <td>0.938737</td>\n",
       "      <td>0.942065</td>\n",
       "      <td>0.942223</td>\n",
       "      <td>0.941022</td>\n",
       "      <td>0.943101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "B         23        32        35        36        37        50        53   \\\n",
       "A                                                                           \n",
       "23   0.940140  0.934226  0.940871  0.946093  0.940307  0.940192  0.941881   \n",
       "32   0.934226  0.928966  0.934924  0.940397  0.934481  0.934343  0.936011   \n",
       "35   0.940871  0.934924  0.941590  0.946788  0.941022  0.940913  0.942596   \n",
       "36   0.946093  0.940397  0.946788  0.952274  0.946297  0.946138  0.947873   \n",
       "37   0.940307  0.934481  0.941022  0.946297  0.940486  0.940379  0.942053   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "424  0.937239  0.931490  0.937931  0.943127  0.937473  0.937313  0.938978   \n",
       "425  0.940583  0.934711  0.941308  0.946538  0.940759  0.940645  0.942326   \n",
       "426  0.940746  0.934883  0.941468  0.946726  0.940910  0.940801  0.942487   \n",
       "428  0.939575  0.933594  0.940312  0.945495  0.939703  0.939618  0.941296   \n",
       "435  0.941619  0.935780  0.942332  0.947633  0.941787  0.941676  0.943365   \n",
       "\n",
       "B         55        57        59   ...       411       416       419  \\\n",
       "A                                  ...                                 \n",
       "23   0.940328  0.938246  0.940784  ...  0.941057  0.940519  0.940024   \n",
       "32   0.934490  0.932356  0.934927  ...  0.935213  0.934719  0.934431   \n",
       "35   0.941047  0.938968  0.941507  ...  0.941788  0.941233  0.940735   \n",
       "36   0.946284  0.944138  0.946763  ...  0.947053  0.946523  0.946100   \n",
       "37   0.940523  0.938426  0.940951  ...  0.941227  0.940697  0.940256   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "424  0.937447  0.935388  0.937896  ...  0.938154  0.937660  0.937137   \n",
       "425  0.940779  0.938703  0.941237  ...  0.941506  0.940979  0.940513   \n",
       "426  0.940945  0.938826  0.941386  ...  0.941676  0.941113  0.940623   \n",
       "428  0.939755  0.937674  0.940198  ...  0.940489  0.939917  0.939436   \n",
       "435  0.941820  0.939712  0.942263  ...  0.942546  0.942002  0.941553   \n",
       "\n",
       "B         420       421       424       425       426       428       435  \n",
       "A                                                                          \n",
       "23   0.939312  0.926623  0.937239  0.940583  0.940746  0.939575  0.941619  \n",
       "32   0.933643  0.920515  0.931490  0.934711  0.934883  0.933594  0.935780  \n",
       "35   0.940030  0.927362  0.937931  0.941308  0.941468  0.940312  0.942332  \n",
       "36   0.945386  0.932182  0.943127  0.946538  0.946726  0.945495  0.947633  \n",
       "37   0.939516  0.926735  0.937473  0.940759  0.940910  0.939703  0.941787  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "424  0.936508  0.923919  0.934393  0.937663  0.937884  0.936693  0.938737  \n",
       "425  0.939784  0.927082  0.937663  0.941037  0.941178  0.940019  0.942065  \n",
       "426  0.939936  0.927098  0.937884  0.941178  0.941380  0.940159  0.942223  \n",
       "428  0.938680  0.926062  0.936693  0.940019  0.940159  0.939000  0.941022  \n",
       "435  0.940827  0.927970  0.938737  0.942065  0.942223  0.941022  0.943101  \n",
       "\n",
       "[76 rows x 76 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantum_fidelity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8db0ffce-a682-43e4-968b-b99164a34de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9342262766708186)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantum_fidelity_matrix[23][32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ac69a-762d-4225-8e4d-9d93322f9b43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# TODO\n",
    "\n",
    "- memoize functions\n",
    "- run on GPU\n",
    "- run on QPU (no mitigation)\n",
    "- run on QPU (with Mitiq)\n",
    "- end to end analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
